{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your data from Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.nyt_dump\n",
    "coll = db.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [' '.join(article['content']).lower() for article in coll.find()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Pipeline \n",
    "A text processing pipeline involves tokenization, stripping stopwords, and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. possible text mining inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "input_string = remove_accents(paragraph)\n",
    "input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized = [word_tokenize(content.lower()) for content in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if word not in stop]\n",
    "            for words in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stemming/Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "docs_snowball = [[snowball.stem(word) for word in words] for words in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Bag Of Words and TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Create your vocab, a set of words UNIQUE  over the whole corpusmy_docs = docs_snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in my_docs]\n",
    "vocab = list(vocab_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Create a reverse lookup for the vocab list.\n",
    " This is a dictionary whose keys are the words and values are the indices of the words (the word id). This will make things much faster than using the list `index` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Now let's create our word count vectors manually.\n",
    "Create a numpy matrix where each row corresponds to a document and each column a word. The value should be the count of the number of times that word appeared in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.zeros((len(my_docs), len(vocab)))\n",
    "for doc_id, words in enumerate(my_docs):\n",
    "    for word in words:\n",
    "        word_id = vocab_dict[word]\n",
    "        word_counts[doc_id][word_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Create the document frequencies. \n",
    "For each word, get a count of the number of documents the word appears in (different from the number of times the word appears!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.sum(word_counts > 0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Normalize the word count matrix to get the term frequencies. \n",
    "This means dividing each count by the L1 norm (the sum of all the counts). This makes each vector a vector of term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_norm = word_counts.sum(axis=1)\n",
    "    tf_norm[tf_norm == 0] = 1\n",
    "    tf = word_counts / tf_norm.reshape(len(my_docs), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Write the tokenize function. \n",
    "It should use nltk's word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "        '''\n",
    "        INPUT: string\n",
    "        OUTPUT: list of strings\n",
    "\n",
    "        Tokenize and stem/lemmatize the document.\n",
    "        '''\n",
    "        return [snowball.stem(word) for word in word_tokenize(doc.lower())]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply the CountVectorizer \n",
    "You can use vect.get_feature_names() to get the ids of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "countvect = CountVectorizer(stop_words='english',tokenizer=tokenize)\n",
    "count_vectorized = countvect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 4, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn count of 'dinner': 2\n",
      "my count of 'dinner': 2.0\n"
     ]
    }
   ],
   "source": [
    "# Compare my results:\n",
    "\n",
    "words = countvect.get_feature_names()\n",
    "print(\"sklearn count of 'dinner':\", count_vectorized[0, words.index('dinner')])\n",
    "print(\"my count of 'dinner':\", word_counts[0, vocab_dict['dinner']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Apply the TfidfVectorizer. Compare it to your tfidf results from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn tfidf of 'dinner': 0.0572326357301963\n"
     ]
    }
   ],
   "source": [
    "words_tfidf = tfidfvect.get_feature_names()\n",
    "print(\"sklearn tfidf of 'dinner':\", tfidf_vectorized[0, words_tfidf.index('dinner')])\n",
    "# print(\"my tfidf of 'dinner':\", tfidf[0, vocab_dict['dinner']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Centroids from K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Import Data and apply KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_pickle(\"data/articles.pkl\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(articles_df['content'])\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Print centroids as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cluster centers:\")\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Find the top 10 features for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "print(\"\\n3) top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(f\"{num}, {', '.join(features[i] for i in centroid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Set k = # sections. Find and count sections for each group\n",
    "Not a perfect map to each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X)\n",
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "     Business Day (34 articles)\n",
      "     Opinion (11 articles)\n",
      "     World (9 articles)\n",
      "     U.S. (7 articles)\n",
      "     Sports (1 articles)\n",
      "     Arts (1 articles)\n",
      "Cluster 1:\n",
      "     Arts (89 articles)\n",
      "     Opinion (3 articles)\n",
      "     Business Day (2 articles)\n",
      "     World (1 articles)\n",
      "Cluster 2:\n",
      "     World (30 articles)\n",
      "     Business Day (2 articles)\n",
      "     U.S. (1 articles)\n",
      "Cluster 3:\n",
      "     World (22 articles)\n",
      "     Opinion (3 articles)\n",
      "     U.S. (1 articles)\n",
      "Cluster 4:\n",
      "     World (145 articles)\n",
      "     U.S. (6 articles)\n",
      "     Opinion (4 articles)\n",
      "     Business Day (2 articles)\n",
      "     Sports (2 articles)\n",
      "Cluster 5:\n",
      "     Sports (30 articles)\n",
      "Cluster 6:\n",
      "     Sports (92 articles)\n",
      "     World (3 articles)\n",
      "     Arts (1 articles)\n",
      "     Opinion (1 articles)\n",
      "     Business Day (1 articles)\n",
      "Cluster 7:\n",
      "     Sports (72 articles)\n",
      "     Arts (1 articles)\n",
      "Cluster 8:\n",
      "     Business Day (27 articles)\n",
      "     Arts (24 articles)\n",
      "     Sports (23 articles)\n",
      "     Opinion (13 articles)\n",
      "     World (5 articles)\n",
      "     U.S. (1 articles)\n",
      "Cluster 9:\n",
      "     U.S. (28 articles)\n",
      "     Business Day (2 articles)\n",
      "     Opinion (1 articles)\n"
     ]
    }
   ],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster==i]\n",
    "    topics = articles_df.loc[cluster].dropna()['section_name']\n",
    "    most_common = Counter(topics).most_common()\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for j in range (len(most_common)):\n",
    "        print(f\"     {most_common[j][0]} ({most_common[j][1]} articles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
